{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMB0x/VSgwD1vHfyqi/0h4I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Machine Learning with PyTorch: Building an SPY Price Forecasting System"],"metadata":{"id":"JqPC2jj-XPZP"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pY02uneVXhjr","executionInfo":{"status":"ok","timestamp":1748937528061,"user_tz":360,"elapsed":21929,"user":{"displayName":"Zhiqi Pang (Jenny)","userId":"04514025054656742098"}},"outputId":"8bf879c5-032f-4832-8c08-21a80607b1d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader, TensorDataset\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","import os\n","import matplotlib.dates as mdates\n","from matplotlib.gridspec import GridSpec\n","\n","# --- Custom Early Stopping Implementation ---\n","class EarlyStopping:\n","    def __init__(self, patience=20, min_delta=0):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.counter = 0\n","        self.best_loss = float('inf')\n","        self.early_stop = False\n","\n","    def check(self, val_loss, model):\n","        if val_loss < self.best_loss - self.min_delta:\n","            self.best_loss = val_loss\n","            self.counter = 0\n","        else:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","\n","# --- File Paths ---\n","data_path = os.getenv('DATA_PATH', './default_data/spy_sample.csv')\n","model_path = os.getenv('MODEL_PATH', './saved_models/model.pth')\n","plots_dir = os.getenv('PLOTS_DIR', './outputs/figures')\n","\n","# Create directories\n","os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n","os.makedirs(plots_save_dir, exist_ok=True)\n","\n","# --- Data Loading and Preprocessing ---\n","print(\"Loading data...\")\n","df = pd.read_csv(data_file_path, parse_dates=['timestamp'])\n","df = df.sort_values('timestamp').reset_index(drop=True)\n","\n","# Data quality check\n","print(\"Checking data quality...\")\n","print(f\"NaN values in raw data: {df.isna().sum().sum()}\")\n","print(f\"Infinite values in raw data: {np.isinf(df.select_dtypes(include=np.number)).sum().sum()}\")\n","\n","# --- Technical Indicators Calculation ---\n","def calculate_atr(df, window=14):\n","    high_low = df['high'] - df['low']\n","    high_close = np.abs(df['high'] - df['close'].shift())\n","    low_close = np.abs(df['low'] - df['close'].shift())\n","    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n","    atr = tr.rolling(window).mean()\n","    return atr\n","\n","def calculate_rsi(df, window=14):\n","    delta = df['close'].diff()\n","    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n","    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n","    rs = gain / loss\n","    rsi = 100 - (100 / (1 + rs))\n","    return rsi\n","\n","def calculate_macd(df, fast=12, slow=26, signal=9):\n","    ema_fast = df['close'].ewm(span=fast, adjust=False).mean()\n","    ema_slow = df['close'].ewm(span=slow, adjust=False).mean()\n","    macd = ema_fast - ema_slow\n","    signal_line = macd.ewm(span=signal, adjust=False).mean()\n","    return macd, signal_line\n","\n","def calculate_ma(df, windows=[5, 10, 20]):\n","    ma = {}\n","    for window in windows:\n","        ma[f'MA{window}'] = df['close'].rolling(window).mean()\n","    return ma\n","\n","def calculate_volatility(df, window=14):\n","    returns = np.log(df['close']).diff()  # Log returns for better volatility calculation\n","    volatility = returns.rolling(window).std() * np.sqrt(window)\n","    return volatility\n","\n","# Calculate all technical indicators\n","print(\"Calculating technical indicators...\")\n","df['ATR14'] = calculate_atr(df, 14)\n","df['RSI14'] = calculate_rsi(df, 14)\n","macd, signal = calculate_macd(df)\n","df['MACD'] = macd\n","df['MACD_Signal'] = signal\n","ma_dict = calculate_ma(df, [5, 10, 20])\n","for key, value in ma_dict.items():\n","    df[key] = value\n","df['Volatility%'] = calculate_volatility(df, 14) * 100\n","df['LogClose'] = np.log(df['close'])  # Add log transformation\n","\n","# Handle NaN values\n","df.bfill(inplace=True)\n","df.ffill(inplace=True)\n","\n","# Data validation\n","print(f\"NaN values after processing: {df.isna().sum().sum()}\")\n","print(f\"Infinite values after processing: {np.isinf(df.select_dtypes(include=np.number)).sum().sum()}\")\n","\n","# --- Prepare Model Data ---\n","feature_cols = ['open', 'high', 'low', 'close', 'volume', 'ATR14', 'Volatility%', 'RSI14', 'MACD', 'MACD_Signal', 'MA5', 'MA10', 'MA20']\n","\n","def create_windowed_data(df, feature_cols, window_size=6):\n","    X, y = [], []\n","    for i in range(window_size, len(df)):\n","        window_data = df.loc[i - window_size:i - 1, feature_cols].values.flatten()\n","        if not np.any(np.isnan(window_data)) and not np.any(np.isinf(window_data)):\n","            X.append(window_data)\n","            y.append(df.loc[i, 'LogClose'])  # Using log price as target\n","    return np.array(X), np.array(y)\n","\n","X, y = create_windowed_data(df, feature_cols, window_size=6)\n","print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n","\n","# Data scaling\n","scaler_X = StandardScaler()\n","X_scaled = scaler_X.fit_transform(X)\n","\n","scaler_y = StandardScaler()\n","y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, shuffle=False)\n","\n","# Convert to tensors\n","X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n","X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n","\n","# Create DataLoader\n","train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=64, shuffle=True)\n","val_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=64, shuffle=False)\n","\n","# --- Model Definition ---\n","class EnhancedMLP(nn.Module):\n","    def __init__(self, input_dim):\n","        super(EnhancedMLP, self).__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, 128),\n","            nn.BatchNorm1d(128),\n","            nn.LeakyReLU(0.01),\n","            nn.Dropout(0.2),\n","            nn.Linear(128, 64),\n","            nn.BatchNorm1d(64),\n","            nn.LeakyReLU(0.01),\n","            nn.Linear(64, 1))\n","\n","    def forward(self, x):\n","        if torch.isnan(x).any() or torch.isinf(x).any():\n","            x = torch.nan_to_num(x)\n","        return self.net(x)\n","\n","# --- Custom Loss Function ---\n","class DirectionalLoss(nn.Module):\n","    def __init__(self, alpha=0.5):\n","        super().__init__()\n","        self.alpha = alpha  # Weight for directional component\n","        self.mse = nn.MSELoss()\n","\n","    def forward(self, pred, target):\n","        # Standard MSE component\n","        mse_loss = self.mse(pred, target)\n","\n","        # Directional component\n","        pred_diff = pred[1:] - pred[:-1]\n","        target_diff = target[1:] - target[:-1]\n","        directional_loss = torch.mean(1 - torch.sign(pred_diff * target_diff).float()) / 2\n","\n","        # Combined loss\n","        return (1 - self.alpha) * mse_loss + self.alpha * directional_loss\n","\n","model = EnhancedMLP(input_dim=X_train.shape[1])\n","loss_fn = nn.HuberLoss(delta=1.0)  # Using Huber loss for robustness\n","# Alternatively: loss_fn = DirectionalLoss(alpha=0.3)  # Custom directional loss\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n","early_stopping = EarlyStopping(patience=20)\n","\n","# --- Training Loop ---\n","num_epochs = 300\n","train_losses = []\n","val_losses = []\n","best_loss = float('inf')\n","\n","print(\"Starting training...\")\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","\n","    for xb, yb in train_loader:\n","        optimizer.zero_grad()\n","        pred = model(xb)\n","        loss = loss_fn(pred, yb)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    # Validation phase\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for xb, yb in val_loader:\n","            pred = model(xb)\n","            val_loss += loss_fn(pred, yb).item()\n","\n","    epoch_loss = running_loss/len(train_loader)\n","    val_loss = val_loss/len(val_loader)\n","    train_losses.append(epoch_loss)\n","    val_losses.append(val_loss)\n","\n","    scheduler.step(val_loss)\n","\n","    # Early stopping check\n","    early_stopping.check(val_loss, model)\n","    if early_stopping.early_stop:\n","        print(f\"Early stopping triggered at epoch {epoch+1}!\")\n","        break\n","\n","    if val_loss < best_loss:\n","        best_loss = val_loss\n","        torch.save(model.state_dict(), model_save_path)\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.6f} - Val Loss: {val_loss:.6f}\")\n","\n","# Load best model\n","model.load_state_dict(torch.load(model_save_path))\n","print(f\"Best model loaded with validation loss: {best_loss:.6f}\")\n","\n","# --- Plot Training Loss ---\n","plt.figure(figsize=(10, 5))\n","plt.plot(train_losses, label='Training Loss')\n","plt.plot(val_losses, label='Validation Loss')\n","plt.title('Training & Validation Loss Curve')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.grid(True)\n","loss_plot_path = os.path.join(plots_save_dir, 'training_loss.png')\n","plt.savefig(loss_plot_path)\n","plt.close()\n","print(f\"Training loss plot saved to: {loss_plot_path}\")\n","\n","# --- Model Evaluation ---\n","model.eval()\n","with torch.no_grad():\n","    y_pred_scaled = model(X_test_tensor).squeeze().numpy()\n","\n","# Inverse scaling and transform\n","y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n","y_test = scaler_y.inverse_transform(y_test_tensor.numpy().reshape(-1, 1)).flatten()\n","\n","# Convert from log space to price\n","y_pred_actual = np.exp(y_pred)\n","y_test_actual = np.exp(y_test)\n","\n","# Calculate directional accuracy\n","direction_acc = ((np.diff(y_pred) * np.diff(y_test)) > 0).mean()\n","\n","# Metrics\n","mse = mean_squared_error(y_test_actual, y_pred_actual)\n","mae = mean_absolute_error(y_test_actual, y_pred_actual)\n","print(f\"\\n✅ Directional Accuracy: {direction_acc:.2%}\")\n","print(f\"✅ Final MSE: {mse:.4f}\")\n","print(f\"✅ Final MAE: {mae:.4f}\")\n","\n","# --- Visualization ---\n","test_indices = range(len(df) - len(y_test), len(df))\n","test_timestamps = df['timestamp'].iloc[test_indices].values\n","\n","def plot_prediction_with_indicators(timestamps, y_true, y_pred, df_subset):\n","    fig = plt.figure(figsize=(16, 12))\n","    gs = GridSpec(4, 1, figure=fig)\n","\n","    # Price prediction vs actual\n","    ax1 = fig.add_subplot(gs[0, 0])\n","    ax1.plot(timestamps, y_true, label='Actual Close', color='blue', linewidth=1.5)\n","    ax1.plot(timestamps, y_pred, label='Predicted Close', color='red', linestyle='--', linewidth=1)\n","    ax1.set_title('Price Prediction vs Actual')\n","    ax1.legend()\n","    ax1.grid(True)\n","    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))\n","\n","    # RSI\n","    ax2 = fig.add_subplot(gs[1, 0])\n","    ax2.plot(timestamps, df_subset['RSI14'].values, label='RSI14', color='purple')\n","    ax2.axhline(70, color='red', linestyle='--', alpha=0.3)\n","    ax2.axhline(30, color='green', linestyle='--', alpha=0.3)\n","    ax2.set_title('RSI (14-period)')\n","    ax2.legend()\n","    ax2.grid(True)\n","\n","    # MACD\n","    ax3 = fig.add_subplot(gs[2, 0])\n","    ax3.plot(timestamps, df_subset['MACD'].values, label='MACD', color='blue')\n","    ax3.plot(timestamps, df_subset['MACD_Signal'].values, label='Signal', color='orange')\n","    ax3.axhline(0, color='black', linestyle='-', alpha=0.3)\n","    ax3.set_title('MACD (12,26,9)')\n","    ax3.legend()\n","    ax3.grid(True)\n","\n","    # ATR\n","    ax4 = fig.add_subplot(gs[3, 0])\n","    ax4.plot(timestamps, df_subset['ATR14'].values, label='ATR14', color='green')\n","    ax4.set_title('Average True Range (ATR)')\n","    ax4.legend()\n","    ax4.grid(True)\n","\n","    plt.tight_layout()\n","    return fig\n","\n","# Generate and save plots\n","df_test_subset = df.iloc[test_indices].copy()\n","fig = plot_prediction_with_indicators(test_timestamps, y_test_actual, y_pred_actual, df_test_subset)\n","prediction_plot_path = os.path.join(plots_save_dir, 'prediction_with_indicators.png')\n","fig.savefig(prediction_plot_path)\n","plt.close(fig)\n","print(f\"Prediction plot saved to: {prediction_plot_path}\")\n","\n","# Save metrics\n","metrics_file = os.path.join(plots_save_dir, 'metrics.txt')\n","with open(metrics_file, 'w') as f:\n","    f.write(f\"MSE: {mse:.4f}\\n\")\n","    f.write(f\"MAE: {mae:.4f}\\n\")\n","    f.write(f\"Directional Accuracy: {direction_acc:.2%}\\n\")\n","print(f\"Metrics saved to: {metrics_file}\")\n","\n","print(\"Training and evaluation completed!\")"],"metadata":{"id":"AYfwon8iTFG2"},"execution_count":null,"outputs":[]}]}